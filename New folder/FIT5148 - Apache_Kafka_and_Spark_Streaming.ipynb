{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4QUZ40Cg2IK"
   },
   "source": [
    "# FIT5148 - Big data management and processing\n",
    "\n",
    "# Activity: Apache Kafka, Spark Streaming, MongoDB and Visualization#\n",
    "\n",
    "In this activity, we are going to discuss Apache Kafka and how Spark streaming programmers can use it for building distributed systems. Apache Kafka® is a distributed streaming platform. It is an open-source streaming platform that was initially built by LinkedIn. It was later handed over to Apache foundation and open sourced it in 2011.\n",
    "\n",
    "## 1. Overview ##\n",
    "\n",
    "### 1.1 What is Apache Kafka?\n",
    "\n",
    "According to Wikipedia:\n",
    "\n",
    "Apache Kafka is an open-source stream-processing software platform developed by the Apache Software Foundation, written in Scala and Java. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Its storage layer is essentially a “massively scalable pub/sub message queue architected as a distributed transaction log,” making it highly valuable for enterprise infrastructures to process streaming data. Additionally, Kafka connects to external systems (for data import/export) via Kafka Connect and provides Kafka Streams, a Java stream processing library.\n",
    "\n",
    "Think of it is a big commit log where data is stored in sequence as it happens. The users of this log can just access and use it as per their requirement.\n",
    "\n",
    "### 1.2 Kafka Use Cases\n",
    "Uses of Kafka are multiple. Here are a few use-cases that could help you to figure out its usage.\n",
    "\n",
    "- #### Messaging\n",
    "Kafka can be used as a message broker among services. If you are implementing a microservice architecture, you can have a microservice as a producer and another as a consumer. For instance, you have a microservice that is responsible to create new accounts and other for sending email to users about account creation.\n",
    "\n",
    "- #### Activity Monitoring\n",
    "Kafka can be used for activity monitoring. The activity could belong to a website or physical sensors and devices. Producers can publish raw data from data sources that later can be used to find trends and pattern.\n",
    "\n",
    "- #### Log Aggregation\n",
    "Kafka can be used to collect logs from different systems and store in a centralized system for further processing.\n",
    "\n",
    "- #### Extract Transform and Load (ETL)\n",
    "Kafka has a feature of almost real-time streaming thus you can come up with an ETL based on your need.\n",
    "\n",
    "- #### Database \n",
    "Kafka can also acts as a database. It is not a typical databases that have a feature of querying the data as per need, but you can keep data in Kafka as long as you want without consuming it (Message queing).\n",
    "\n",
    "### 1.3 Kafka Concepts\n",
    "\n",
    "Let’s discuss some core Kafka concepts.\n",
    "\n",
    "![Overview of Kafka](https://cdn-images-1.medium.com/max/1600/1*48ck-bvatKzEpVapVa4Mag.png)\n",
    "\n",
    "#### Topics\n",
    "Every message that is feed into the system must be part of some *topic*. The topic is nothing but a stream of records. The messages are stored in *key-value* format. Each message is assigned a sequence, called *Offset*. The output of one message could be an input of the other for further processing.\n",
    "\n",
    "#### Producers\n",
    "*Producers* are the applications responsible to publish data into Kafka system. They publish data on the *topic* of their choice.\n",
    "\n",
    "#### Consumers\n",
    "The messages published into *topics* are then utilized by *Consumer* applications. A *consumer* gets subscribed to the *topic* of its choice and consumes data.\n",
    "\n",
    "#### Broker\n",
    "Every instance of Kafka that is responsible for message exchange is called a *Broker*. Kafka can be used as a stand-alone machine or a part of a cluster.\n",
    "\n",
    "### 1.4 Setting up and Running\n",
    "\n",
    "The easiest way to install Kafka is to download binaries and run it. Since it’s based on JVM languages like Scala and Java, you must make sure that you are using Java 7 or greater. \n",
    "If you want to setup Apache Kafka in your personal machine, you can find the tutorial [here](https://kafka.apache.org/quickstart).\n",
    "\n",
    "*Note: For simplicity, the virtual machines (VM) provided to the students and used in the labs have been configured to run Apache Kafka during the start up of the operating system. *\n",
    "\n",
    "### 1.5 Accessing Kafka in Python\n",
    "There are multiple python libraries available to connect Apache Kafka and Python together. We are going to use \n",
    "an open-source community-based library known as [Kafka-Python](https://github.com/dpkp/kafka-python).\n",
    "\n",
    "You can install the library using the command below:\n",
    ">  **!pip3 install kafka-python**\n",
    "\n",
    "*Note: This is a python library that facilitates accessing Apache Kafka Producer and Consumer. We will see how we can access Kafka Producer from Apache Spark Streaming in the next section.*\n",
    "\n",
    "\n",
    "Now, we will look into different scenarios on handling data streams as they come. We are not going to process or store data, but just display the data on the fly.\n",
    "\n",
    "**Scenario #01:** The incoming data has uniform arrival. The data comes, and we display it in the graph. For simplicity, we assume that the incoming data is within a specified range, so it will be easy to prepare the display because we know the max (and the min) of y-axis and the x-axis is the arrival time. This is a line graph.\n",
    "\n",
    "Run the following files in sequential order to see the real-time plotting in action.\n",
    "- Week11_Kafka_Producer01.ipynb \n",
    "- Week11_Kafka_Consumer01.ipynb\n",
    "\n",
    "**Scenario #02:** We want to improve the graph in Scenario 01 by showing the label of some interesting points such as minimum and maximum values.\n",
    "\n",
    "Run the following files in sequential order to see the real-time plotting in action.\n",
    "- Week11_Kafka_Producer02.ipynb \n",
    "- Week11_Kafka_Consumer02.ipynb\n",
    "\n",
    "**Scenario #03:** Scenario 01 and 02 assumes that the incoming data is uniform interms of their arrivals. Now, how about if the incoming data is bursty. The interval between data is not uniform. The *creation time* in the producer will be different from the *arrival time* in the consumer. \n",
    "\n",
    "Run the following files in sequential order to see the real-time plotting in action.\n",
    "- Week11_Kafka_Producer03.ipynb \n",
    "- Week11_Kafka_Consumer03.ipynb\n",
    "\n",
    "\n",
    "**Scenario #04:** We want to do some simple processing of the incoming data on the fly.  The data source is still one but we want to show the second line graph which is the value of moving window (such as moving average). So the moving average of the current time is the average value of, for example, the previous 5 values. \n",
    "\n",
    "Run the following files in sequential order to see the real-time plotting in action.\n",
    "- Week11_Kafka_Producer04.ipynb \n",
    "- Week11_Kafka_Consumer04.ipynb\n",
    "\n",
    "**Scenario #05:** Now lets consider we have multiple data sources (say for example 3 producers). We want to plot data from all three producers as well as average of data from all three producers at the given time. So, in total, we will have 4 graphs. \n",
    "\n",
    "Run the following files in sequential order to see the real-time plotting in action.\n",
    "- Week11_Kafka_Producer05.ipynb \n",
    "- Week11_Kafka_Consumer05.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jBeqWJyo4ig2"
   },
   "source": [
    "## 2. Overall Architecture: Kafka and Spark Streaming##\n",
    "\n",
    "The figure below shows the high-level example of a real-time data pipeline that will make use of popular tools including Apache Kafka for message passing, Apache for Spark for data processing, and one of the many data storage tools (e.g. MongoDB) that eventually feeds into internal or external facing products (websites, dashboards etc…).\n",
    "\n",
    "![Apache kafka and Apache Spark Pipeline](http://tlfvincent.github.io/img/kafka_spark_pipeline.png)\n",
    "\n",
    "This will we will basically learn how to receive streams from Apache Kafka producer, handle the missing data and insert into MongoDB. Apache Kafka producers produces the data every second but some of the data will be missing. Apache Spark streaming checks whether we have missing data or not. If we have missing data, we represent it with asterisk ('\\*') and insert into MongoDB.\n",
    "\n",
    "Run the following files in sequential order to see the real-time data pipeline in action.\n",
    "- Week11_Kafka_Producer06.ipynb\n",
    "- Week11_Spark_Streaming.ipnyb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nUJmIAozrmez"
   },
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations on finishing this activity!\n",
    "\n",
    "<font color='blue'>\n",
    "**Wrap up what we've learned:**\n",
    "- Learned that Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results\n",
    "- Learned that using \"StreamingContext\", we can define the input sources by creating input DStreams; apply transformation and output operations to DStreams; and receive data and process it.\n",
    "- Learned how to use \"dstream.foreachRDD\" that allows data to be sent out to external systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OZkeIqcCrpwo"
   },
   "source": [
    "## References\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Apache_Kafka\n",
    "- https://kafka.apache.org/\n",
    "- https://towardsdatascience.com/getting-started-with-apache-kafka-in-python-604b3250aa05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g5Iq7baRg0Mi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FIT5148 - Apache_Kafka_and_Spark_Streaming.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
